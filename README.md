# Sign Language Recognition System

A real-time **ASL / FSL sign language recognition and learning system** with:
- ðŸ¤– **ML Recognition** â€” CNN + Random Forest ensemble (96%+ accuracy)
- ðŸŽ¯ **AR Learning Modes** â€” 4 augmented reality modes for practice
- ðŸ“Š **Real-time Feedback** â€” Accuracy scoring with visual guidance
- ðŸ—„ï¸ **Database-backed** â€” Supabase for landmark storage
- ðŸŽ¨ **3D Visualization** â€” Interactive PyVista hand models

Built as a complete thesis project demonstrating computer vision, machine learning, and augmented reality.

---

## ðŸ“‹ Table of Contents

- [Requirements](#requirements)
- [Quick Setup](#quick-setup)
- [Detailed Setup Guide](#detailed-setup-guide)
- [How to Run](#how-to-run)
- [Project Structure](#project-structure)
- [Features](#features)
- [Troubleshooting](#troubleshooting)
- [For Thesis/Academic Use](#for-thesisacademic-use)

---

## Requirements

| Requirement | Notes |
|-------------|-------|
| **Python 3.10** | 3.9 / 3.11 may work but untested |
| **Webcam** | Any USB or built-in webcam |
| **Supabase account** | Free tier sufficient ([sign up](https://supabase.com)) |
| **Dataset** | ASL/FSL images organized by letter/sign |
| **~4 GB disk space** | For dependencies + TensorFlow |
| **OS** | Windows 10+, macOS 10.14+, or Linux |

**Optional (for ARuco mode):**
- Printer for ARuco marker (can use phone screen as alternative)

**Optional (for phone-anchored AR):**
- YOLO model: `pip install ultralytics`

---

## Quick Setup

**For the impatient:**

```bash
# 1. Clone and enter directory
git clone https://github.com/YOUR_USERNAME/sign-language-system.git
cd sign-language-system

# 2. Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Configure Supabase
cp .env.example .env
# Edit .env with your Supabase credentials

# 5. Setup database
# Copy schema.sql contents to Supabase SQL Editor and run

# 6. Ingest dataset
python -m tools.ingest_dataset --path ./dataset --language ASL --dry-run
python -m tools.ingest_dataset --path ./dataset --language ASL

# 7. Train models
python -m tools.train_models --language ASL

# 8. Run!
python main.py
```

Continue reading for detailed instructions...

---

## Detailed Setup Guide

### Step 1: Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/sign-language-system.git
cd sign-language-system
```

---

### Step 2: Create Virtual Environment

A virtual environment keeps this project's packages isolated.

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**macOS / Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

You should see `(venv)` at the start of your terminal prompt.

**To deactivate later:**
```bash
deactivate
```

---

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**This installs:**
- OpenCV (4.8+) â€” Computer vision
- MediaPipe (0.10.9) â€” Hand tracking
- TensorFlow (2.15.0) â€” CNN model
- scikit-learn â€” Random Forest classifier
- PyVista â€” 3D visualization
- Supabase client â€” Database connection
- Ultralytics (optional) â€” YOLO for phone detection

**Verify installation:**
```bash
python -c "import cv2, mediapipe, tensorflow, sklearn, pyvista, supabase; print('âœ“ All core packages OK')"
```

**Optional (for phone AR):**
```bash
pip install ultralytics
python -c "from ultralytics import YOLO; print('âœ“ YOLO installed')"
```

---

#### 4c. Add Credentials to Config

```python
# config.py
SUPABASE_URL = "https://your-project.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5c..."
```

---

### Step 7: Train ML Models

```bash
python -m tools.train_models --language ASL
```

**This trains:**
1. **CNN** â€” Convolutional Neural Network for pattern recognition
2. **Random Forest** â€” Ensemble classifier for final prediction

**Training process:**
```
[Training] Loading data from Supabase...
[Training] Loaded 22,570 samples (26 classes)
[Training] Normalizing landmarks (palm-scale)...
[Training] Applying mirror augmentation (2Ã— data)...
[Training] Training CNN...
Epoch 1/50: loss=2.4123, acc=0.3421
...
Epoch 50/50: loss=0.1234, acc=0.9614
âœ“ CNN trained (96.14% accuracy)

[Training] Training Random Forest...
âœ“ Random Forest trained (97.23% accuracy)

[Training] Testing ensemble...
âœ“ Ensemble accuracy: 96.87%

Models saved:
  - models/cnn_asl.h5
  - models/rf_asl.pkl
```

**Training time:**
- Small dataset (1K samples): 2-5 minutes
- Medium dataset (10K samples): 10-20 minutes
- Large dataset (50K samples): 30-60 minutes

**Optional: Hyperparameter tuning**
```bash
python -m tools.train_models --language ASL --tune
```

Finds best Random Forest parameters (adds 10-15 minutes).

**Verify models exist:**
```bash
ls models/
# Should show: cnn_asl.h5, rf_asl.pkl
```

---

### Step 8: Generate ARuco Marker (Optional)

Only needed if using ARuco AR mode:

```bash
python -m tools.generate_aruco
```

**Output:**
```
âœ“ ARuco marker generated: aruco_marker_id0.png

Printing instructions:
  1. Open aruco_marker_id0.png
  2. Print at 10cm x 10cm size
  3. Place on flat surface
```

---

## How to Run

### Main Application

```bash
python main.py
```

**You'll see:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sign Language Recognition System    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. ASL (American Sign Language)     â”‚
â”‚ 2. FSL (Filipino Sign Language)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Choose language, then:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Sign â†’ Word (Recognition)        â”‚
â”‚ 2. Word â†’ Sign (Learning)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Mode 1: Sign â†’ Word (Recognition)

**Recognize signs in real-time**

```bash
python main.py â†’ ASL â†’ Sign â†’ Word
```

**Features:**
- Live camera feed
- Real-time predictions (30 FPS with frame-skip caching)
- Top-3 prediction bars with confidence
- Caption history (holds signs for 2 seconds)
- Recording capability (R key)

**Controls:**
- **R** â€” Start/stop recording
- **ESC** â€” Exit

**Performance:**
- Frame skip: Every 3rd frame processed
- FPS: 25-28 (smooth)
- Accuracy: 96%+ (on test set)

---

### Mode 2: Word â†’ Sign (Learning)

**Learn signs with AR guidance**

```bash
python main.py â†’ ASL â†’ Word â†’ Sign
```

**Four AR modes available:**

#### A. 2D Hand AR (Fastest)
- 2D skeleton overlay
- Floats above your hand
- Yellow skeleton = reference
- Green skeleton = your hand
- 30 FPS

#### B. 3D Mesh AR (Phone-Anchored)
- Full 3D mesh
- Anchored to detected phone (YOLO)
- Smooth tracking
- WASD rotation controls
- 20-25 FPS

**Requirements:**
```bash
pip install ultralytics  # For YOLO phone detection
```

**Controls:**
- **A/D** â€” Rotate left/right
- **W/S** â€” Tilt up/down
- **Q/E** â€” Zoom in/out
- **R** â€” Reset view
- **ESC** â€” Exit

#### C. ARuco AR (Most Precise) [EXPERIMENTAL]
- Marker-based (print required)
- Sub-pixel accuracy (Â±1 pixel)
- Academic gold standard
- Same controls as phone AR

**Setup:**
```bash
python -m tools.generate_aruco  # Generate marker
# Print at 10cm Ã— 10cm
```

#### D. 3D Viewer (Study Mode)
- Separate PyVista 3D window
- Rotatable with mouse
- Accuracy feedback in camera window
- Best for detailed study

---

### Utility Scripts

#### Check Database Connection

```bash
python -m tools.check_database
```

Shows:
- Connection status
- Table statistics
- Sample counts per language

#### Ingest Dataset

```bash
# Dry run (test)
python -m tools.ingest_dataset --path ./dataset --language ASL --dry-run

# Real ingestion
python -m tools.ingest_dataset --path ./dataset --language ASL

# Different language
python -m tools.ingest_dataset --path ./fsl_dataset --language FSL
```

#### Train Models

```bash
# Standard training
python -m tools.train_models --language ASL

# With hyperparameter tuning
python -m tools.train_models --language ASL --tune

# For FSL
python -m tools.train_models --language FSL
```

#### Generate ARuco Marker

```bash
python -m tools.generate_aruco
```

Creates `aruco_marker_id0.png` (500Ã—500 pixels with border)

---

## Project Structure

```
sign-language-system/
â”‚
â”œâ”€â”€ main.py                      # Entry point
â”œâ”€â”€ config.py                    # Supabase credentials (GITIGNORED!)
â”œâ”€â”€ .env                         # Environment variables (GITIGNORED!)
â”œâ”€â”€ .env.example                 # Template for .env
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ schema.sql                   # Database schema
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ core/                        # Core logic
â”‚   â”œâ”€â”€ caption.py               # Caption history management
â”‚   â”œâ”€â”€ db.py                    # Database operations
â”‚   â”œâ”€â”€ inference.py             # ML inference with caching
â”‚   â”œâ”€â”€ models.py                # Model loading/training
â”‚   â”œâ”€â”€ recognition.py           # Landmark normalization
â”‚   â””â”€â”€ recording.py             # Video recording
â”‚
â”œâ”€â”€ modes/                       # Application modes
â”‚   â”œâ”€â”€ sign_to_word.py          # Recognition mode
â”‚   â”œâ”€â”€ word_to_sign.py          # Learning mode router
â”‚   â”œâ”€â”€ word_to_sign_hand_2d.py  # 2D skeleton AR
â”‚   â”œâ”€â”€ word_to_sign_hand_3d_final.py  # 3D phone AR
â”‚   â”œâ”€â”€ word_to_sign_aruco.py    # ARuco marker AR
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ui/                          # User interface
â”‚   â”œâ”€â”€ dialogs.py               # Tkinter dialogs
â”‚   â”œâ”€â”€ overlays.py              # OpenCV overlays
â”‚   â”œâ”€â”€ word_picker.py           # Letter/word selection
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ ar/                          # AR utilities
â”‚   â”œâ”€â”€ aligner.py               # Similarity calculation
â”‚   â”œâ”€â”€ mesh_renderer.py         # 3D mesh to 2D image
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ visualization/               # 3D visualization
â”‚   â”œâ”€â”€ hand_3d_combined.py      # PyVista 3D viewer
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tools/                       # Utility scripts
â”‚   â”œâ”€â”€ check_database.py        # Verify DB connection
â”‚   â”œâ”€â”€ ingest_dataset.py        # Load images to DB
â”‚   â”œâ”€â”€ train_models.py          # Train ML models
â”‚   â”œâ”€â”€ generate_aruco.py        # Generate marker
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ models/                      # Trained models (GITIGNORED!)
â”‚   â”œâ”€â”€ cnn_asl.h5
â”‚   â”œâ”€â”€ rf_asl.pkl
â”‚   â”œâ”€â”€ cnn_fsl.h5
â”‚   â””â”€â”€ rf_fsl.pkl
â”‚
â”œâ”€â”€ .cache/                      # Cached data (GITIGNORED!)
â”‚   â””â”€â”€ dataset_*.pkl
â”‚
â””â”€â”€ recordings/                  # Saved recordings (GITIGNORED!)
    â””â”€â”€ recording_*.avi
```

**Total:** ~3,800 lines of Python across 25 files

---

## Features

### Recognition (Sign â†’ Word)

- âœ… Real-time hand detection (MediaPipe)
- âœ… CNN + Random Forest ensemble
- âœ… Frame-skip caching (3Ã— FPS boost)
- âœ… Top-3 predictions with confidence
- âœ… Caption history (2-second hold)
- âœ… Recording capability
- âœ… 96%+ accuracy

### Learning (Word â†’ Sign)

- âœ… 4 AR modes (2D, 3D Phone, ARuco, Viewer)
- âœ… Real-time accuracy feedback
- âœ… Cosine similarity scoring
- âœ… Hand-anchored visualization
- âœ… Interactive controls (WASD, QE, R)
- âœ… Smooth tracking algorithms

### Data Pipeline

- âœ… Supabase integration
- âœ… Automated ingestion
- âœ… Palm-scale normalization
- âœ… Mirror augmentation
- âœ… Disk caching

### ML Pipeline

- âœ… CNN architecture (Conv2D + Dense)
- âœ… Random Forest ensemble
- âœ… Hyperparameter tuning
- âœ… Confusion matrix evaluation
- âœ… Class-balanced training

---

## Troubleshooting

### Installation Issues

**"No module named 'cv2'"**
```bash
pip install opencv-python opencv-contrib-python
```

**"TensorFlow not found"**
```bash
pip install tensorflow==2.15.0
```

**"MediaPipe version conflict"**
```bash
pip install mediapipe==0.10.9
```

### Database Issues

**"Failed to connect to Supabase"**
- Check `.env` file has correct credentials
- Verify `SUPABASE_URL` starts with `https://`
- Verify `SUPABASE_KEY` is service_role key (not anon)
- Test connection: `python -m tools.check_database`

**"Table 'landmark_samples' does not exist"**
- Run `schema.sql` in Supabase SQL Editor
- Refresh browser
- Check database â†’ Tables in dashboard

### Training Issues

**"No samples found"**
- Run ingestion first: `python -m tools.ingest_dataset`
- Check database: `python -m tools.check_database`
- Verify samples uploaded (should show count > 0)

**"Model accuracy very low (<50%)"**
- Dataset too small (need 500+ samples per class)
- Poor quality images (hands not visible)
- Inconsistent hand poses in dataset
- Try with known-good dataset first (see Step 6a)

### Runtime Issues

**"Camera not found"**
- Check webcam connected
- Check permissions (Windows: Settings â†’ Privacy â†’ Camera)
- Try different camera index in code (change `cv2.VideoCapture(0)` to `(1)`)

**"Low FPS / Laggy"**
- Increase frame skip: Edit `sign_to_word.py` line 49: `frame_skip=5`
- Reduce resolution: Add `frame = cv2.resize(frame, (640, 480))`
- Close other camera apps

**"Phone not detected" (3D Phone AR)**
```bash
pip install ultralytics
```

**"ARuco marker not detected"**
- Print at correct size (10cm Ã— 10cm)
- Ensure marker flat (no curves)
- Good lighting (no shadows, glare)
- Update OpenCV: `pip install --upgrade opencv-python`

---

## Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

